# Apache Flink Environment for Kafka Integration
# Based on Confluent's "Building Apache Flink Applications in Java" course
# Usage: docker-compose -f kafka-single-node.yml -f docker-compose.flink.yml up

version: '3.8'

services:
  # Flink JobManager - Coordinates and schedules Flink jobs
  flink-jobmanager:
    image: flink:1.18.1-java11  # Latest stable version with Java 11
    container_name: flink-learning-jobmanager
    hostname: flink-jobmanager
    restart: unless-stopped
    
    # Resource configuration optimized for your system
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    
    ports:
      - "8082:8081"  # Flink Web UI (avoiding conflict with Kafka UI on 8081)
      - "6123:6123"  # JobManager RPC port
    
    command: jobmanager
    
    environment:
      # JobManager Configuration
      - "FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager"
      - "JOB_MANAGER_RPC_ADDRESS=flink-jobmanager"
      
      # Memory settings optimized for learning environment
      - jobmanager.memory.process.size=1600m
      - jobmanager.memory.flink.size=1280m
      - jobmanager.memory.jvm-overhead.min=128m
      - jobmanager.memory.jvm-overhead.max=128m
      
      # Parallelism settings (based on your 8-core system)
      - parallelism.default=4
      - taskmanager.numberOfTaskSlots=4
      
      # Checkpointing configuration for fault tolerance
      - execution.checkpointing.interval=10s
      - execution.checkpointing.mode=EXACTLY_ONCE
      - execution.checkpointing.timeout=300s
      
      # State backend configuration (for learning - filesystem)
      - state.backend=filesystem
      - state.checkpoints.dir=file:///tmp/flink-checkpoints
      - state.savepoints.dir=file:///tmp/flink-savepoints
      
      # Web UI configuration
      - web.submit.enable=true
      - web.upload.dir=/tmp/flink-web-uploads
      
      # Kafka integration settings
      - metrics.reporters=prom
      - metrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter
      - metrics.reporter.prom.host=flink-jobmanager
      - metrics.reporter.prom.port=9250-9260
    
    volumes:
      - flink_checkpoints:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - flink_uploads:/tmp/flink-web-uploads
      - ./flink-jobs:/opt/flink/usrlib  # For job JAR files
    
    networks:
      - default
    
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/overview || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Flink TaskManager - Executes the actual data processing
  flink-taskmanager:
    image: flink:1.18.1-java11
    container_name: flink-learning-taskmanager
    hostname: flink-taskmanager
    restart: unless-stopped
    
    # Resource configuration for TaskManager
    deploy:
      resources:
        limits:
          cpus: '4.0'      # More CPU for data processing
          memory: 3G
        reservations:
          cpus: '2.0'
          memory: 1.5G
    
    command: taskmanager
    
    environment:
      # TaskManager Configuration
      - "FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager"
      - "JOB_MANAGER_RPC_ADDRESS=flink-jobmanager"
      
      # Memory settings for TaskManager
      - taskmanager.memory.process.size=2560m
      - taskmanager.memory.flink.size=2048m
      - taskmanager.memory.managed.fraction=0.4
      - taskmanager.memory.network.fraction=0.1
      - taskmanager.memory.jvm-overhead.min=256m
      - taskmanager.memory.jvm-overhead.max=256m
      
      # Task slots configuration
      - taskmanager.numberOfTaskSlots=4
      - taskmanager.slot.timeout=30s
      
      # Network configuration
      - taskmanager.network.memory.buffers-per-channel=2
      - taskmanager.network.memory.floating-buffers-per-gate=8
      
      # Metrics and monitoring
      - metrics.reporters=prom
      - metrics.reporter.prom.class=org.apache.flink.metrics.prometheus.PrometheusReporter
      - metrics.reporter.prom.host=flink-taskmanager
      - metrics.reporter.prom.port=9250-9260
    
    volumes:
      - flink_checkpoints:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - ./flink-jobs:/opt/flink/usrlib
    
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    
    networks:
      - default
    
    # Scale TaskManagers if needed
    scale: 1

  # Flink SQL Client - Interactive SQL interface for Flink (Confluent Course Compatible)
  flink-sql-client:
    image: flink:1.18.1-java11
    container_name: flink-learning-sql-client
    hostname: flink-sql-client
    restart: "no"  # Manual start for interactive sessions
    
    # Resource configuration for SQL Client
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    
    command: >
      bash -c "
        echo 'Setting up Flink SQL Client for Confluent Course...'
        
        # Download and setup flink-faker connector if not present
        if [ ! -f /opt/flink/lib/flink-faker-0.5.3.jar ]; then
          echo 'Downloading flink-faker connector...'
          wget -q -O /opt/flink/lib/flink-faker-0.5.3.jar \
            https://github.com/knaufk/flink-faker/releases/download/v0.5.3/flink-faker-0.5.3.jar || \
          curl -L -o /opt/flink/lib/flink-faker-0.5.3.jar \
            https://github.com/knaufk/flink-faker/releases/download/v0.5.3/flink-faker-0.5.3.jar
        fi
        
        echo 'Waiting for Flink JobManager to be ready...'
        until curl -s http://flink-jobmanager:8081/overview > /dev/null 2>&1; do
          sleep 2
        done
        
        echo ''
        echo 'ðŸŽ“ Confluent Apache Flink 101 Course Environment Ready!'
        echo '=================================================='
        echo 'Flink SQL Client is starting...'
        echo 'Use these commands to follow the course:'
        echo ''
        echo '1. Switch to batch mode:    SET '\''execution.runtime-mode'\'' = '\''batch'\'';'
        echo '2. Switch to stream mode:   SET '\''execution.runtime-mode'\'' = '\''streaming'\'';'
        echo '3. Change result display:   SET '\''sql-client.execution.result-mode'\'' = '\''changelog'\'';'
        echo '4. Load course scripts:     \\. /opt/flink/sql-scripts/course-exercises.sql'
        echo '5. Get help:                help;'
        echo '6. Exit:                    quit;'
        echo '=================================================='
        echo ''
        
        /opt/flink/bin/sql-client.sh
      "
    
    environment:
      # SQL Client Configuration
      - "FLINK_PROPERTIES=jobmanager.rpc.address: flink-jobmanager"
      - "JOB_MANAGER_RPC_ADDRESS=flink-jobmanager"
      
      # Table API settings for SQL
      - table.exec.resource.default-parallelism=2
      - table.exec.sink.not-null-enforcer=DROP
      - table.optimizer.join-reorder-enabled=true
      
      # SQL Client specific settings (course defaults)
      - sql-client.execution.result-mode=table
      - sql-client.execution.max-table-result.rows=10000
      - execution.runtime-mode=streaming
    
    volumes:
      - flink_checkpoints:/tmp/flink-checkpoints
      - flink_savepoints:/tmp/flink-savepoints
      - ./flink-sql:/opt/flink/sql-scripts  # For SQL script files
      - ./flink-jobs:/opt/flink/usrlib
    
    depends_on:
      flink-jobmanager:
        condition: service_healthy
    
    networks:
      - default
    
    stdin_open: true
    tty: true
    
    profiles:
      - sql  # Optional service - enable with --profile sql

  # Kafka Connect for advanced integrations (optional)
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    container_name: kafka-learning-connect
    hostname: kafka-connect
    restart: unless-stopped
    
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    
    ports:
      - "8083:8083"  # Connect REST API
    
    environment:
      # Connect worker configuration
      CONNECT_BOOTSTRAP_SERVERS: kafka:29092
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      
      # Connect cluster configuration
      CONNECT_GROUP_ID: kafka-connect-learning-group
      CONNECT_CONFIG_STORAGE_TOPIC: _kafka-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: _kafka-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: _kafka-connect-status
      
      # Topic configuration
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      
      # Converter configuration for JSON data
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: false
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: false
      
      # Internal converter configuration
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: false
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: false
      
      # Plugin path for connectors
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components
      
      # Logging configuration
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR
      
      # JVM settings
      KAFKA_HEAP_OPTS: "-Xmx768M -Xms768M"
    
    volumes:
      - kafka_connect_data:/tmp/kafka-connect
      - ./connectors:/usr/share/confluent-hub-components  # For custom connectors
    
    depends_on:
      kafka:
        condition: service_healthy
    
    networks:
      - default
    
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/connectors || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 45s
    
    profiles:
      - connect  # Optional service - enable with --profile connect

  # PostgreSQL connector for Kafka Connect (when using Connect profile)
  postgres-connector-setup:
    image: curlimages/curl:latest
    container_name: postgres-connector-setup
    restart: "no"
    
    command: >
      sh -c "
        echo 'Waiting for Kafka Connect to be ready...'
        until curl -f http://kafka-connect:8083/connectors; do
          sleep 5
        done
        
        echo 'Creating PostgreSQL source connector...'
        curl -X POST http://kafka-connect:8083/connectors \
        -H 'Content-Type: application/json' \
        -d '{
          \"name\": \"postgres-source-connector\",
          \"config\": {
            \"connector.class\": \"io.confluent.connect.jdbc.JdbcSourceConnector\",
            \"connection.url\": \"jdbc:postgresql://host.docker.internal:5432/mydata\",
            \"connection.user\": \"cnldev\",
            \"connection.password\": \"cnldev_123\",
            \"table.whitelist\": \"kafka_message\",
            \"mode\": \"incrementing\",
            \"incrementing.column.name\": \"id\",
            \"topic.prefix\": \"postgres-\",
            \"poll.interval.ms\": 10000
          }
        }'
        
        echo 'PostgreSQL connector setup complete!'
      "
    
    depends_on:
      kafka-connect:
        condition: service_healthy
    
    networks:
      - default
    
    profiles:
      - connect

volumes:
  flink_checkpoints:
    driver: local
    name: flink-learning-checkpoints
  
  flink_savepoints:
    driver: local
    name: flink-learning-savepoints
  
  flink_uploads:
    driver: local
    name: flink-learning-uploads
  
  kafka_connect_data:
    driver: local
    name: kafka-connect-learning-data

networks:
  default:
    external: true
    name: kafka-learning-network

# Usage Instructions:
#
# 1. Basic Flink setup (JobManager + TaskManager):
#    docker-compose -f kafka-single-node.yml -f docker-compose.flink.yml up
#
# 2. Include Kafka Connect:
#    docker-compose -f kafka-single-node.yml -f docker-compose.flink.yml --profile connect up
#
# 3. Access points:
#    - Flink Web UI: http://localhost:8082
#    - Kafka Connect API: http://localhost:8083
#    - Kafka UI: http://localhost:8081
#
# 4. Flink job deployment:
#    Place your JAR files in ./flink-jobs/ directory
#    Submit via Web UI or REST API
